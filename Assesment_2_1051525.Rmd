---
title: "Penguins_assessment_1051525"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

#### URL link to Github repository:

https://github.com/Ihrtcode/Penguins_1051525


### Reproducibility crisis

More than 70% of researchers have failed to reproduce another scientists experiment according to a survey by Nature (Baker, 2016) . This is a key sign of the reproducibility crisis, a growing issue in science impacting the scientific method, and the consequential reproducibility of scientific studies. The surfacing of this as a current issue has resulted in the undermining of the credibility of a large proportion of published studies, leading to significant losses of valuable scientific knowledge (Davey, 2022). 
It is driven by a multitude of different causes, including publication bias, journal impact factors and general bad scientific practise. The major impacts of this can be seen in both the scientific community and the wider public; however, actions  such as the introduction of open access practices have been taken to effectively combat this in the future, slowly reshaping the way publication is viewed.

A major driver of the reproducibility crisis is publication bias, where published studies are statistically skewed to report significant results. This leads to the literature not providing a representative sample of the true evidence, leading to distorted results from meta-analyses which are important in practices such as evidence-based medicine (Song, et al., 2010). Evidence for this can be taken from a 1987 paper estimating that papers with statistically significant results are three times as likely to be accepted and published than those with null results (K, S, & T, 1987); the potency of this effect has increased significantly in the last 10 years. A promising practice for tackling the crisis is pre-registration of proposed journals.

Journal impact factors identify the yearly average number of citations of a paper, and can be used as a tool by universities, and funding bodies to advise employment and promotions (Baker, 2016). They have recently come under attack due to their effect of discouraging good scientific practise by providing strong motivation for scientists to produce significant results under the ‘publish or perish’ mentality (Davey, 2022). As a result, researchers may feel forced to resort to data-dredging (Davey, 2022) or other malpractice to advance their careers.

The peer review process has come under scrutiny , with open-access journals combating the reproducibility crisis by introducing transparent peer review processes. This could lead to higher quality reviews compared to closed-review, however myths that OA journals have lower quality peer review processes persist, leading to potential discouragement for researchers to publish in journals considered less prestigious. This barrier must be overcome with a shift in the outlook of the scientific community on open access journals and publications. 

A significant consequence of the reproducibility crisis is that it can lead to the formulation and testing of future hypotheses which are based on false impressions from published literature. This can result in researchers time along with limited research and grant opportunities being wasted. (Shields, 2000). 
Additionally,  it can be considered unethical from the standpoint of taxpayers whose money is being used for publicly funded research as it’s arguably immoral to waste this contribution or not give open access to the resultant research (McKiernan, 2016).

###### Bibliography
Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533, 452-454.

Davey, R. (2022). What is the Replication Crisis? Retrieved from News medical and life sciences: https://www.news-medical.net/life-sciences/What-is-the-Replication-Crisis.aspx

K, D., S, C., & T, C. (1987). Publication bias and clinical trials. Controlled clinical trials, 343-353.

McKiernan, E. C. (2016). Point of View: How open science helps researchers succeed . Retrieved from eLife: https://elifesciences.org/articles/16800#bib126

Shields, P. G. (2000). Publication Bias Is a Scientific Problem with Adverse Ethical Outcomes: The Case for a Section for Null Results . Cancer Epidemiology, Biomarkers & Prevention , 771-772.

Song, F., Parekh, S., Hooper, L., Loke, Y. K., Ryder, J., Sutton, A. J., . . . Harvey, I. (2010). "Dissemination and publication of research findings: An updated review of related biases. Health Technology Assessment, 14, 193.


```{r, echo = FALSE, include = FALSE}
source("scripts/import_clean.r")
source("scripts/import_image.r")
source("scripts/analysis.r")
source("scripts/badplot.r")
source("scripts/penguinplot1.r")

```

### Bad scientific communication
```{r badplot, echo=FALSE}
badplot
```

### Bad plot

Figure 1: Culmen length and sex as predictors of body mass is a difficult to interpret figure for a number of reasons. Firstly, instead of starting at 0, the Y-axis starts at an undefined value just below 3,000. This may be misleading in showing the spread of the data , leading to a misinterpretation of the difference between the means.  
Secondly the photograph of the penguin behind the graph is visually very distracting, whilst it does immediately show the focus of the dataset (penguins), it takes attention away from the violin plot and what the graph is aiming to communicate. Furthermore, it is an image of an Adelie penguin, therefore suggesting that the data is only relevant to individuals from this species, whereas it is stated in figure legend that it includes data from all 3 species in the dataset. The mean value for each island is indicated by a large green star. The green against the green and red in particular may make the graph inaccessible to people with red-green vision deficiency. Furthermore, the star is too large and due to its unusual shape, it is difficult to determine where the middle point is to identify the mean value.


### Colour in scientific communication


Colour choice is an essential tool for effective communication in scientific figures, with reference to 3 main categories: feature identification, exploration, and communication (S.Zeller, 2020).  Furthermore, digital publishing has increased the prevalence of colour in scientific communication. However, colour schemes and gradients can be misused in a variety of ways which can result in; the inaccessibility of scientific figures to people with colour-vision deficiencies, the manipulation of how a dataset may be interpreted, and the loss of information if a figure is printed in black and white. On top of these, one must consider cultural and semantic associations of colour with regard to scientific figures in the media, which is becoming increasingly widespread in mainstream culture (Fabio Crameri, 2020).

The aim of data presentation generally falls under the 3 categories listed above. Firstly, scientists may look to identify specific features of a dense dataset which are relevant to a given hypothesis. Alternatively, the visualisation may be used to explore the data holistically and make general observations and hypotheses. Lastly, and most commonly, it may be used to communicate specific properties or trends of a dataset to peers, the public or other audiences (S.Zeller, 2020). Colour is a key tool used to enhance these aims.

It’s estimated that 8% of men worldwide are subject to colour-vision deficiencies. Colour combinations ill-suited to such people, such as red-green contrasts, can provide a significant barrier in accessing scientific communications. For example, weather maps, climate change and global pandemic figures are all common forms of scientific communication in global mainstream media which should be made universally accessible where possible. However, figures with low contrast may make it difficult to distinguish different groups on legends. Therefore, additional tools such as differing fill patterns and shape outlines along with online colour blindness visualisation tools can be utilized to aid differentiation (Timothy B. Plante MD, 2020). 

Colour gradients provide the greatest topic for discourse in the misuse of colour for communication. The rainbow colour gradient, despite its aesthetic attractiveness, is associated with a range of limitations, including inaccessibility to red-green colour blindness, a non-uniform gradient, and yellow being the brightest colour despite being middle of the range, leading to subconscious distortion of data values (Fabio Crameri, 2020). Therefore, research into more appropriate colour schemes which are resistant to conversion to grey-scale, and avoid misleading distortion is necessary.

Whilst procuring  an optimal uniform colour gradient for different dataset profiles may be appealing, this runs a risk of becoming monotonous and boring, losing the interest of viewers. Therefore, new tools such as ‘ColorMoves’ have been developed which are widely accessible and can fine-tune colormaps to the needs of different datasets, maximising visualisation of fine-details and discriminatory power whilst using semantic association to optimise the communication of the data, for example, green for land, or red for hot (S.Zeller, 2020).
Furthermore, colour also has strong symbolism within cultures and politics which should be considered with reference to how interpretations may vary on a global scale (Nerlich, 2013). For example, green is widely considered positively and red negatively, whereas other colours may be perceived differently. 


###### Bibliography

Fabio Crameri, G. E. (2020). The misuse of colour in science communication. Nature, 5444.

Nerlich, B. (2013). Making science public: A question of colour. Retrieved from University of Nottingham: https://blogs.nottingham.ac.uk/makingsciencepublic/2013/09/16/making-science-public-a-question-of-colour/

S.Zeller, D. R. (2020). Visualizing Science: How Color Determines What We See. Retrieved from Eos: https://eos.org/features/visualizing-science-how-color-determines-what-we-see

Timothy B. Plante MD, M. C. (2020). Choosing color palettes for scientific figures. Research and Practice in Thrombosis and Haemostasis, 4(2), 176-180.



## Statistical test and plot 

### Diagnostic plots
```{r, echo = FALSE, message=FALSE}
analysis1
```

#### Normal Q-Q plot
Points fall roughly along the straight diagonal line, with minor tailing off at either end, however, this does not violate the assumption that the residuals are normally distributed

```{r, echo=FALSE, message=FALSE}
analysis2
```

#### Scale-location plot
The line is roughly horizontal line with a deviation at the right end;  the points are  relatively evenly spread, suggesting acceptable homogeneity with no significant violation of equal variance. 

### ANOVA statistical test
##### ANOVA table
```{r, echo=FALSE}
anova
```

```{r penguinplot1, echo=FALSE, message = FALSE}
#plot graph and save to as pdf in figures
penguinplot1
```

URL link to partners work on Github:

https://github.com/anonymousoxstudent/penguins/blob/main/penguin2.R

#### Analysis of code
The code is clearly split into distinct, labelled sections. Within each section most functions and other lines of code have been annotated, making the code logical and easy to follow. 
When I attempted to run the code, there were a couple of errors that arose. Firstly, as instructed I added my own working directory. However, there was then an error with write.csv function as no file was found.
However, as penguins_raw is part of the palmerpenguins package, the write.csv function isn’t necessary and the code runs regardless. This made subsequent functions such as dir.create() redundant. 

The code runs smoothly, with all expected outputs until line 101 where the plot_grid function isn’t recognised. Once I edited the code to load the cowplot packge, which wasn’t in the library, the plot_grid function ran without errors, and the object total_plot was created. 
The figure then successfully saved to the figures folder of my chosen working directory as an SVG file.
The final figure produced penguins_vector.svg is clear, with 2 separate graphs for both male and female penguins. However, the scale of both axis of the graph varies between the 2 plots, which may be misleading. Additionally, a figure caption or annotation within the R document explaining how the  statistical test links to the graph would be informative. 

To make the code more understandable it would be beneficial to convert it into Rmarkdown file which can produce a clean output with only the necessary information displayed. This would increase focus on the statistical test and final figure. Additionally, they would benefit from using the source function and multiple files for functions, plots, and figures to reduce the volume of code on the main output document, allowing each of these parts to be easily run separately, making it easier to identify errors and edit.

The code is relatively easy to edit due to the thorough annotations, however, it would be significantly easier if it were split into separate files which could be sourced into the main output document as described above. This way smaller sections could be run independently and the one could make test scripts to easily run only specific functions from the sourced files at a time. 

My partner suggested adding the setwd() fucntion so one can easily connect the working directory to where the files are, and smoothly source the functions as they could not run the code without this. They suggested that having all the code in the same Rmd file would make it easier to download and run from Github. I agree with the working directory amendment, however, I believe sourcing of different files should make the code more reproducible as it is easier to edit, especially when working with longer scripts and analyses. 

My main takeaway for writing code for others is the value of detailed annotation as whilst I could not originally run my partners code, their annotation made it easy for me to understand and edit. Ordering code logically and dividing it into clear sections are also important for reproducibility.


